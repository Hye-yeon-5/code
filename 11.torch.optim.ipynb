{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "任务名称：优化算法torch.optim操作10个方法[参考资料：pytorch官方文档torch.optim\n",
    "(2)PyTorch_tutorial_0.0.5_余霆嵩文档45~56页]\n",
    "任务简介：pytorch中的优化算法\n",
    "详细说明：需要了解常用优化算法的优势和劣势，同时需要了解如何调参，参数中学习率是非常重要的参数，学习率设置不合适会导致震荡或者收敛很慢。\n",
    "作业名称（详解）：对同一网络进行不同优化算法的测试，比对其效果和收敛速度\n",
    "作业提交形式：PPT截图或手写拍照,打卡提交.\n",
    "打卡内容：（可以只是文字提交，或图片提交，或组合都行）\n",
    "文字  要求最少   字\n",
    "音频  要求最短   秒\n",
    "图片  要求最少  10~15 张\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.optim:\n",
    "torch.optim 是一个实现了各种优化算法的库。大部分常用的方法得到支持，并且接口具备足 够的通用性，使得未来能够集成更加复杂的方法。\n",
    "\n",
    "如何使用optimizer:\n",
    "为了使用 torch.optim ，你需要构建一个optimizer对象。这个对象能够保持当前参数状态并基 于计算得到的梯度进行参数更新。\n",
    "\n",
    "构建:\n",
    "为了构建一个 Optimizer ，你需要给它一个包含了需要优化的参数 的iterable。然后，你可以设置optimizer的参 数选项，比如学习率，权重衰减，等等\n",
    "\n",
    "例子：\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) \n",
    "optimizer = optim.Adam([var1, var2], lr = 0.0001) \n",
    "\n",
    "为每个参数单独设置选项:\n",
    "Optimizer 也支持为每个参数单独设置选项。若想这么做，不要直接传入 iterable，而是传入 dict 的iterable。每一个dict都分别定 义了一组参数，并且包含一个 param 键，这个键对应参数的列表。其他的键应该optimizer所接受的其他参数的关键字相匹配，并 且会被用于对这组参数的 优化。\n",
    "\n",
    "注意：\n",
    "你仍然能够传递选项作为关键字参数。在未重写这些选项的组中，它们会被用作默认值。当 你只想改动一个参数组的选项，但其他参数组的选项不变时，这是 非常有用的。\n",
    "例如，当我们想指定每一层的学习率时，这是非常有用的：\n",
    "optim.SGD([{'params': model.base.parameters()}, {'params': model.classifier.parameters()], 'lr': 1e-3} ,lr=1e-2, momentum=0.9) \n",
    "这意味着 model.base 的参数将会使用 1e-2 的学习率， model.classifier 的参数将会使用 1e-3 的学习率，并且 0.9 的momentum将会被用于所 有的参数。\n",
    "\n",
    "进行单次优化:\n",
    "所有的optimizer都实现了 step() 方法，这个方法会更新所有的参数。它能按两种方式来使 用：\n",
    "optimizer.step()\n",
    "这是大多数optimizer所支持的简化版本。一旦梯度被如 backward() 之类的函数计算好后，我 们就可以调用这个函数。\n",
    "\n",
    "例子\n",
    "for input, target in dataset:     \n",
    "    optimizer.zero_grad()     \n",
    "    output = model(input)     \n",
    "    loss = loss_fn(output, target)     \n",
    "    loss.backward()     \n",
    "    optimizer.step() \n",
    "    optimizer.step(closure)\n",
    "一些优化算法例如Conjugate Gradient和LBFGS需要重复多次计算函数，因此你需要传入一个 闭包去允许它们重新计算你的模型。这个闭包应当清空梯度， 计算损失，然后返回。\n",
    "\n",
    "例子：\n",
    "for input, target in dataset:     \n",
    "    def closure():        \n",
    "        optimizer.zero_grad()        \n",
    "        output = model(input)        \n",
    "        loss = loss_fn(output, target)         \n",
    "        loss.backward()         \n",
    "        return loss     \n",
    "        optimizer.step(closure) \n",
    "        \n",
    "\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "算法\n",
    "class torch.optim.Optimizer(params, defaults) [source]\n",
    "Base class for all optimizers.\n",
    "参数：\n",
    "params (iterable) —— dict 的iterable。指定了什么参数应当被优化。 \n",
    "defaults —— (dict)：包含了优化选项默认值的字典（一个参数组没有指定的参数选项将会 使用默认值）。\n",
    "\n",
    "load_state_dict(state_dict) [source]\n",
    "加载optimizer状态\n",
    "参数：\n",
    "state_dict ( dict ) —— optimizer的状态。应当是一个调用 state_dict() 所返回的对象。\n",
    "state_dict() [source]\n",
    "以 dict 返回optimizer的状态。\n",
    "它包含两项。\n",
    "state - 一个保存了当前优化状态的dict。optimizer的类别不同，state的内容也会不同。 param_groups - 一个包含了全部参数组的dict。\n",
    "step(closure) [source]\n",
    "进行单次优化 (参数更新).\n",
    "参数：\n",
    "closure ( callable ) – 一个重新评价模型并返回loss的闭包，对于大多数参数来说是可选 的。\n",
    "zero_grad() [source]\n",
    "清空所有被优化过的梯度.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adadelta:\n",
    "#class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) 实现Adadelta算法。\n",
    "\n",
    "\"\"\"\n",
    "参数：\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "rho ( float , 可选) – 用于计算平方梯度的运行平均值的系数（默认：0.9） \n",
    "eps ( float , 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-6） \n",
    "lr ( float , 可选) – 在delta被应用到参数更新之前对它缩放的系数（默认：1.0） \n",
    "weight_decay ( float , 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adagrad:\n",
    "\n",
    "#class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0),实现Adagrad算法\n",
    "\"\"\"\n",
    "参数：\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "lr ( float , 可选) – 学习率（默认: 1e-2） \n",
    "lr_decay ( float , 可选) – 学习率衰减（默认: 0） \n",
    "weight_decay ( float , 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adamax:\n",
    "#class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0),实现Adamax算法（Adam的一种基于无穷范数的变种)\n",
    "\"\"\"\n",
    "参数：\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "lr ( float , 可选) – 学习率（默认：2e-3） \n",
    "betas (Tuple[ float , float ], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数 \n",
    "eps ( float , 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）\n",
    "weight_decay ( float , 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam:\n",
    "#class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e08, weight_decay=0)[source]实现Adam算法。\n",
    "\"\"\"\n",
    "参数：\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "lr ( float , 可选) – 学习率（默认：1e-3）\n",
    "betas (Tuple[ float , float ], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默 认：0.9，0.999） \n",
    "eps ( float , 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）\n",
    "weight_decay ( float , 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASGD\n",
    "#class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)[source]\n",
    "#实现平均随机梯度下降算法。\n",
    "\n",
    "\"\"\"\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "lr ( float , 可选) – 学习率（默认：1e-2） \n",
    "lambd ( float , 可选) – 衰减项（默认：1e-4） \n",
    "alpha ( float , 可选) – eta更新的指数（默认：0.75） \n",
    "t0 ( float , 可选) – 指明在哪一次开始平均化（默认：1e6） \n",
    "weight_decay ( float , 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LBFGS:\n",
    "#class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)[source]\n",
    "#实现L-BFGS算法。\n",
    "\"\"\"\n",
    "警告\n",
    "这个optimizer不支持为每个参数单独设置选项以及不支持参数组（只能有一个）\n",
    "警告\n",
    "目前所有的参数不得不都在同一设备上。在将来这会得到改进。\n",
    "注意\n",
    "这是一个内存高度密集的optimizer（它要求额外的 param_bytes * (history_size + 1) 个字 节）。\n",
    "如果它不适应内存，尝试减小history size，或者使用不同的算法。\n",
    "\n",
    "参数：\n",
    "lr ( float ) – 学习率（默认：1） \n",
    "max_iter ( int ) – 每一步优化的最大迭代次数（默认：20）) \n",
    "max_eval ( int ) – 每一步优化的最大函数评价次数（默认：max * 1.25） \n",
    "tolerance_grad ( float ) – 一阶最优的终止容忍度（默认：1e-5） \n",
    "tolerance_change ( float ) – 在函数值/参数变化量上的终止容忍度（默认：1e-9)\n",
    "history_size ( int ) – 更新历史的大小（默认：100）\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop:\n",
    "#class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#实现RMSprop算法\n",
    "\"\"\"\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "lr ( float , 可选) – 学习率（默认：1e-2） \n",
    "momentum ( float , 可选) – 动量因子（默认：0） \n",
    "alpha ( float , 可选) – 平滑常数（默认：0.99） \n",
    "eps ( float , 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） \n",
    "centered ( bool , 可选) – 如果为True，计算中心化的RMSProp，并且用它的方差预测值对 梯度进行归一化 \n",
    "weight_decay ( float , 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rprop:\n",
    "#class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes= (1e-06, 50))[source]\n",
    "#实现弹性反向传播算法\n",
    "\"\"\"\n",
    "\n",
    "参数：\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "lr ( float , 可选) – 学习率（默认：1e-2） \n",
    "etas (Tuple[ float , float ], 可选) – 一对（etaminus，etaplis）, 它们分别是乘法的增加和减 小的因子（默认：0.5，1.2） \n",
    "step_sizes (Tuple[ float , float ], 可选) – 允许的一对最小和最大的步长（默认：1e-6， 50）\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD:\n",
    "#class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#实现随机梯度下降算法（momentum可选）。\n",
    "\n",
    "\"\"\"\n",
    "params (iterable) – 待优化参数的iterable或者是定义了参数组的dict \n",
    "lr ( float ) – 学习率 \n",
    "momentum ( float , 可选) – 动量因子（默认：0）\n",
    "weight_decay ( float , 可选) – 权重衰减（L2惩罚）（默认：0） dampening ( float , 可选) – 动量的抑制因子（默认：0） nesterov ( bool , 可选) – 使用Nesterov动量（默认：False\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以上就是所有优化算法，同学们可以自行测试每个优化算法，只需修改optimizer=optim.优化算法（）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[200/10000], loss: 6.296507\n",
      "Epoch[400/10000], loss: 5.613523\n",
      "Epoch[600/10000], loss: 4.988832\n",
      "Epoch[800/10000], loss: 4.418646\n",
      "Epoch[1000/10000], loss: 3.899473\n",
      "Epoch[1200/10000], loss: 3.428078\n",
      "Epoch[1400/10000], loss: 3.001463\n",
      "Epoch[1600/10000], loss: 2.616824\n",
      "Epoch[1800/10000], loss: 2.271529\n",
      "Epoch[2000/10000], loss: 1.963093\n",
      "Epoch[2200/10000], loss: 1.689146\n",
      "Epoch[2400/10000], loss: 1.447415\n",
      "Epoch[2600/10000], loss: 1.235701\n",
      "Epoch[2800/10000], loss: 1.051854\n",
      "Epoch[3000/10000], loss: 0.893765\n",
      "Epoch[3200/10000], loss: 0.759331\n",
      "Epoch[3400/10000], loss: 0.646473\n",
      "Epoch[3600/10000], loss: 0.553107\n",
      "Epoch[3800/10000], loss: 0.477152\n",
      "Epoch[4000/10000], loss: 0.416536\n",
      "Epoch[4200/10000], loss: 0.369207\n",
      "Epoch[4400/10000], loss: 0.333155\n",
      "Epoch[4600/10000], loss: 0.306444\n",
      "Epoch[4800/10000], loss: 0.287250\n",
      "Epoch[5000/10000], loss: 0.273898\n",
      "Epoch[5200/10000], loss: 0.264906\n",
      "Epoch[5400/10000], loss: 0.259012\n",
      "Epoch[5600/10000], loss: 0.255197\n",
      "Epoch[5800/10000], loss: 0.252684\n",
      "Epoch[6000/10000], loss: 0.250914\n",
      "Epoch[6200/10000], loss: 0.249517\n",
      "Epoch[6400/10000], loss: 0.248266\n",
      "Epoch[6600/10000], loss: 0.247032\n",
      "Epoch[6800/10000], loss: 0.245750\n",
      "Epoch[7000/10000], loss: 0.244388\n",
      "Epoch[7200/10000], loss: 0.242932\n",
      "Epoch[7400/10000], loss: 0.241378\n",
      "Epoch[7600/10000], loss: 0.239724\n",
      "Epoch[7800/10000], loss: 0.237970\n",
      "Epoch[8000/10000], loss: 0.236119\n",
      "Epoch[8200/10000], loss: 0.234176\n",
      "Epoch[8400/10000], loss: 0.232146\n",
      "Epoch[8600/10000], loss: 0.230038\n",
      "Epoch[8800/10000], loss: 0.227860\n",
      "Epoch[9000/10000], loss: 0.225624\n",
      "Epoch[9200/10000], loss: 0.223340\n",
      "Epoch[9400/10000], loss: 0.221022\n",
      "Epoch[9600/10000], loss: 0.218681\n",
      "Epoch[9800/10000], loss: 0.216331\n",
      "Epoch[10000/10000], loss: 0.213984\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],\n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042],\n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573],\n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827],\n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "# Linear Regression Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # input and output is 1 dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "# 定义loss和优化函数\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "# 开始训练\n",
    "num_epochs = 10000  #epoch可自定义\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = x_train\n",
    "    target = y_train\n",
    "\n",
    "    # forward\n",
    "    out = model(inputs)\n",
    "    loss = criterion(out, target)\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print('Epoch[{}/{}], loss: {:.6f}'\n",
    "              .format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "model.eval()\n",
    "predict = model(x_train)\n",
    "predict = predict.data.numpy()\n",
    "plt.plot(x_train.numpy(), y_train.numpy(), 'ro', label='Original data')\n",
    "plt.plot(x_train.numpy(), predict, label='Fitting Line')\n",
    "# 显示图例\n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), './linear.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
